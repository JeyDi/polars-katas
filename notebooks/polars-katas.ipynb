{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "base = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-{:02d}.parquet\"\n",
    "\n",
    "# we keep the files from Feb 2023 to retain those with the same schema\n",
    "urls = tuple(base.format(month) for month in range(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "<details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pdm install -G data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "\n",
    "import requests_cache\n",
    "import polars as pl\n",
    "from retry_requests import retry\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after = -1)\n",
    "retry_session = retry(cache_session, retries = 5, backoff_factor = 0.2)\n",
    "openmeteo = openmeteo_requests.Client(session = retry_session)\n",
    "\n",
    "# Make sure all required weather variables are listed here\n",
    "# The order of variables in hourly or daily is important to assign them correctly below\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "\t\"latitude\": 40.7143,\n",
    "\t\"longitude\": -74.006,\n",
    "\t\"start_date\": \"2023-02-01\",\n",
    "\t\"end_date\": \"2023-11-30\",\n",
    "\t\"hourly\": [\"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\", \"snowfall\", \"snow_depth\", \"weather_code\", \"wind_speed_10m\", \"wind_speed_100m\"]\n",
    "}\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "\n",
    "# Process first location. Add a for-loop for multiple locations or weather models\n",
    "response = responses[0]\n",
    "print(f\"Coordinates {response.Latitude()}Â°E {response.Longitude()}Â°N\")\n",
    "print(f\"Elevation {response.Elevation()} m asl\")\n",
    "print(f\"Timezone {response.Timezone()} {response.TimezoneAbbreviation()}\")\n",
    "print(f\"Timezone difference to GMT+0 {response.UtcOffsetSeconds()} s\")\n",
    "\n",
    "# Process hourly data. The order of variables needs to be the same as requested.\n",
    "hourly = response.Hourly()\n",
    "hourly_temperature_2m = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_relative_humidity_2m = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_precipitation = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_rain = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_snowfall = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_snow_depth = hourly.Variables(5).ValuesAsNumpy()\n",
    "hourly_weather_code = hourly.Variables(6).ValuesAsNumpy()\n",
    "hourly_wind_speed_10m = hourly.Variables(7).ValuesAsNumpy()\n",
    "hourly_wind_speed_100m = hourly.Variables(8).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\"),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\"),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"relative_humidity_2m\"] = hourly_relative_humidity_2m\n",
    "hourly_data[\"precipitation\"] = hourly_precipitation\n",
    "hourly_data[\"rain\"] = hourly_rain\n",
    "hourly_data[\"snowfall\"] = hourly_snowfall\n",
    "hourly_data[\"snow_depth\"] = hourly_snow_depth\n",
    "hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "hourly_data[\"wind_speed_10m\"] = hourly_wind_speed_10m\n",
    "hourly_data[\"wind_speed_100m\"] = hourly_wind_speed_100m\n",
    "\n",
    "hourly_dataframe = pl.DataFrame(data = hourly_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 1: Eager Mode\n",
    "\n",
    "* Read the first parquet file in the list using `pl.read_parquet`.\n",
    "* Display the top five rows.\n",
    "\n",
    "How long does this take?\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "url = urls[0]\n",
    "\n",
    "pl.read_parquet(url).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 2: Lazy mode\n",
    "\n",
    "Repeat the exercise above, using `pl.scan_parquet` instead. What happens if you run the code? What changes if you scan the whole set of URLs?\n",
    "\n",
    "Write the code to materialize the result.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "pl.scan_parquet(url).head().collect()\n",
    "\n",
    "pl.scan_parquet(urls).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 3: The schema\n",
    "\n",
    "Display the `LazyFrame` schema.\n",
    "\n",
    "Like `pandas`, `Polars` can `describe` the dataset. Can you do that on a `LazyFrame`?\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data = pl.scan_parquet(urls)\n",
    "\n",
    "data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 4: Selecting columns\n",
    "\n",
    "Use `pl.select()` context to get the following columns:\n",
    "\n",
    "1. Select all columns\n",
    "2. Select all columns except `VendorID`.\n",
    "3. Select all columns that contain `amount` in their name\n",
    "4. Select all `Int64` columns.\n",
    "5. Select all `Int64` and `Int32` columns.\n",
    "5. Select all numeric columns.\n",
    "6. Select all datetime and string columns, minus the first column.\n",
    "\n",
    "> **Hint**. To inspect the intermediate steps or results of a query, you can always call the `fetch()` method. It is like a debug statement.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data = pl.scan_parquet(urls)\n",
    "\n",
    "# 1. Select all columns\n",
    "data.select(pl.all())\n",
    "data.select(\"*\")\n",
    "# 2. Select all columns except `VendorID`.\n",
    "data.select(pl.all().except(\"VendorID\"))\n",
    "# 3. Select all columns that contain `amount` in their name\n",
    "data.select(pl.col(r\"^*amount$\"))\n",
    "# 4. Select all integer columns.\n",
    "data.select(pl.col(pl.Int64))\n",
    "# 5. Select all numeric columns.\n",
    "data.select(pl.col(pl.NUMERIC_DTYPES))\n",
    "# 6. Select all datetime and string columns.\n",
    "data.select(pl.col(pl.DateTime), pl.col(pl.Utf8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 5: Column Selectors\n",
    "\n",
    "Use `selectors` to implement the previous selections, and the following ones:\n",
    "\n",
    "7. Select all columns that are integers or datetime, except the first one.\n",
    "8. Select all columns that contain an \"ID\" or \"amount\" and are not floating point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars.selectors as cs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. Select all columns\n",
    "data.select(cs.all())\n",
    "# 2. Select all columns except `VendorID`.\n",
    "data.select(~cs.by_name(\"VendorID\"))\n",
    "# 3. Select all columns that contain `amount` in their name\n",
    "data.select(cs.contains(\"amount\"))\n",
    "data.select(cs.matches(\"*amount\"))\n",
    "# 4. Select all integer columns.\n",
    "data.select(cs.integer())\n",
    "# 5. Select all numeric columns.\n",
    "data.select(cs.numeric())\n",
    "# 6. Select all datetime and string columns.\n",
    "data.select(cs.temporal(), cs.string())\n",
    "data.select(cs.temporal() | cs.string())\n",
    "# 7. Select all columns that are integers or datetime, except the first one.\n",
    "data.select(cs.integer() - cs.first() | cs.temporal())\n",
    "# 8. Select all columns that contain an \"ID\" or \"amount\" and are not floating point numbers.\n",
    "data.select(cs.contains((\"ID\", \"Amount\")) | ~cs.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 6: Introduction to expressions\n",
    "\n",
    "We already used expressions, such as `pl.col()` or `pl.all()`. These operations, that start with `pl.`, can only be evaluated inside a context.\n",
    "Outside of one, they can be assigned to a variable or be used as return value of a function and still retain all query optimisations.\n",
    "\n",
    "1. Multiply the `trip_distance` by 1000 to cast it in metres and name it `trip_distance_meters`.\n",
    "2. Add `tolls_amount`, `Airport_fee` and name it `total_fees`.\n",
    "3. Compute the ratio between `tip`, `total_fees`, `mta_tax` and `fare_amount` over `total_amount`.\n",
    "4. Compute the average trip distance.\n",
    "5. Count the different values of `passenger_count`.\n",
    "6. Get the number of unique values of `VendorID` and `RatecodeID`.\n",
    "\n",
    "> **Hint**. You can call `.alias` on an expression to rename the column it generates. Similarly, you can access the `.name.suffix` method to add a suffix. Alternatively, you can name the column using a kwarg notation (i.e., `col=pl.some.expr`).\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# 1. Multiply the `trip_distance` by 1000 to cast it in metres.\n",
    "data.with_columns(trip_distance_meters = pl.col(\"trip_distance\") * 1000)\n",
    "data.with_columns(pl.col(\"trip_distance\").mul(1000).alias(\"trip_distance_meters\"))\n",
    "data.with_columns(pl.col(\"trip_distance\").mul(1000).name.suffix(\"_meters\"))\n",
    "# 2. Add `tolls_amount`, `Airport_fee` and name it `total_fees`.\n",
    "data.with_columns(total_fees=pl.col(\"tolls_amount\") + pl.col(\"Airport_fee\")))\n",
    "# 3. Compute the ratio between `tip`, `mta_tax` and `fare_amount` over `total_amount`.\n",
    "data.with_columns(pl.col(\"tip\", \"mta_tax\", \"fare_amount\").truediv(\"total_amount\").name.suffix(\"_pct\"))\n",
    "# 4. Compute the average trip distance.\n",
    "data.with_columns(pl.col(\"trip_distance\").mean())\n",
    "# 5. Count the unique values of `passenger_count`.\n",
    "data.with_columns(pl.col(\"passenger_count\").value_counts())\n",
    "# 6. Get the unique values of `VendorID` and `RatecodeID`.\n",
    "data.with_columns(pl.col(\"VendorID\", \"RatecodeID\").n_unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 7: The query plan\n",
    "\n",
    "The `LazyFrame` represents a *Logical Plan*, i.e. a sequence of transformations. It embodies a query, rather than a `DataFrame`. You can inspect this plan when you print the `repr` of the `LazyFrame`.\n",
    "\n",
    "* What method does it suggest to call, to inspect the optimized plan?\n",
    "* Inspect the plan of the last exercise of the previous kata, comparing the optimised and unoptimised queries.\n",
    "* If you have `graphviz` on your `$PATH`, do the same with `data.show_graph`.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "percentage_change = pl.col(\"tip\", \"mta_tax\", \"fare_amount\").truediv(\"total_amount\").with_suffix(\"_pct\")\n",
    "data.with_columns(percentage_change).explain(optimized=True)\n",
    "data.with_columns(percentage_change).explain(optimized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 8: Chaining multiple contextes\n",
    "\n",
    "Try to write the expressions in the sixth kata in the same `with_column` context. Do you notice any errors popping up? Can you explain them?\n",
    "\n",
    "Call `explain` on both and compare the query plans.\n",
    "\n",
    "<details>\n",
    "<summary>ðŸ”Ž Solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data.with_columns(\n",
    "    pl.col(\"trip_distance\").mul(1000).name.suffix(\"_meters\"),\n",
    "    pl.col(\"tolls_amount\").add(pl.col(\"Airport_fee\")).alias(\"total_fees\"),\n",
    ").with_columns(\n",
    "    pl.col(\"tip_amount\", \"total_fees\", \"mta_tax\", \"fare_amount\").truediv(pl.col(\"total_amount\")).name.suffix(\"_pct\")\n",
    ").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 9: Data types\n",
    "\n",
    "You can change the memory representation of a numeric datatype with `.cast()`.\n",
    "\n",
    "1. Cast the string column into a categorical.\n",
    "2. Cast the integer columns to have the smallest memory footprint.\n",
    "3. Cast the datetime columns to milliseconds.\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data.select(\n",
    "    cs.temporal().as_expr().cast(pl.Date),\n",
    "    cs.string().as_expr().cast(pl.Categorical),\n",
    "    cs.numeric().shrink_dtype()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 10: Namespaces\n",
    "\n",
    "Polars segregates operations on similar data types behind namespaces, e.g. `str`, `dt`, `list` and `struct`.\n",
    "\n",
    "1. Cast the `store_and_fwd_flag` column to lowercase.\n",
    "2. Extract the year, month and day of the temporal columns.\n",
    "3. Cast the temporal columns to strings.\n",
    "  1. Split them at the ` ` (space) mark\n",
    "  2. Take the first element\n",
    "  3. Split the element at the `-` mark.\n",
    "  4. Cast the result into a struct.\n",
    "  5. Cast the struct into a JSON string with.\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data.select(\n",
    "    cs.temporal().as_expr()\n",
    "    .cast(pl.Utf8)\n",
    "    .str.split(\" \")\n",
    "    .list.first()\n",
    "    .str.split(\"-\")\n",
    "    .list.to_struct()\n",
    "    .struct.json_encode()\n",
    ").fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 10: Filtering\n",
    "\n",
    "Filtering, combined with the query optimiser, can translate in huge gains. Thanks to the so-called \"predicate pushdown\", query engines can scan parquet files to just read the required rows - thus saving bandwidth and I/O.\n",
    "\n",
    "Filtering is done inside the `filter` context and uses basic Python logical operators. Perform the following filtering operations:\n",
    "\n",
    "1. Passenger count is greater than 3.\n",
    "2. The dropoff hour is the same as the pickup's.\n",
    "2. Trip distance is greater than the average trip distance.\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 11: Aggregations\n",
    "\n",
    "Aggregations with `group_by` can be elegantly expressed in Polars. An aggregation looks like this: `data.group_by(...).agg(...)`. Inside the brackets, there can be any expression!\n",
    "\n",
    "1. Aggregate by passenger count and compute the average and standard deviation of the trip distance.\n",
    "2. Aggregate by month and compute the mean and standard deviation of the total price and trip distance.\n",
    "3. Take the logs (base 10) of the price and round it to the nearest tenth. Use this to aggregate by price order of magnitude and compute the average and standard deviation of passengers and trip distance.\n",
    "4. Aggregate by vendor ID and just write `\"passenger_count` inside the `.agg` context. What happens?\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 12: Window Functions\n",
    "\n",
    "Window functions can be powerful allies to generate features. This are especially hard with pandas, since they would require creating a new dataframe and performing a join.\n",
    "\n",
    "Window functions are just computed as this: `pl.col(...).mean().over(...)`. They are especially useful for time-based data. For those cases, you can use the powerful `Expr.rolling()` to perform rolling window computations across datetime columns.\n",
    "\n",
    "1. Compute the mean and standard deviation of the price over vendor ID and passenger count.\n",
    "2. Compute the rolling window of the price over a week.\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 13: Joins\n",
    "\n",
    "1. Load the weather data csv. Pay attention to the headers!\n",
    "2. Cast the data types to a proper format.\n",
    "2. Join the weather on the pickup time column. Use the hour as the join key.\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 14: Join-asof\n",
    "\n",
    "Repeat the join above but use the join-asof.\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "</details>\n",
    "\n",
    "## Kata 15: Manipulating the elements of a list\n",
    "\n",
    "Aggregate the data by vendor and passenger count on trip distance and fare amount. Compute the rolling mean of size 3 over the elements of the list.\n",
    "\n",
    "<details>\n",
    "<summary>solution</summary>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "(\n",
    "    data\n",
    "    .group_by(\"VendorID\", \"passenger_count\")\n",
    "    .agg(\"trip_distance\", \"fare_amount\")\n",
    "    .with_columns(\n",
    "        pl.col(\"trip_distance\").list.eval(pl.element().rolling_mean(window_size=3))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
